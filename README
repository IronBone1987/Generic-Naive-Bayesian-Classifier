A naive Bayesian classifier
Pre Alpha
----
----
Implementation of the naive bayesian classifier formula:
0) train the algorithm with containers that are categorized positive or negative compared to a criteria.
  0.1) tokenize objects within a container(ie. tokenize words within the body of an email)
       (to tokenize means to acquire attributes in smallest meaningful units (ie. strip unwanted 
       words and characters from the body of an email))
  0.2) move the categorized tokens to the respective database table
1) compare new documents with 'trained' database; 
  1.1) using the formulas listed below, calculate each token's probability with each the database table to
       acquire the likelihood of testing positive and negative.
  1.2) upon, binary categorization, move tokenized objects to either of two database tables (ie. spam or not_spam) adding a count of 1 to counter if they already exist. 
  1.3) calculating formulas and training the database can be done independently; that is, neither
       is dependent on the other happening before a method is called.
----
----
Background:
Bayes Therom
P(A|B) = (P(B|A)*P(A))/P(B)

The probability of A given B is equal to the probability of B given A multiplied by the probability of A, divided over the probability of B.

A naive bayesian classifier formula

P(S|W) = P(W|S) / ( P(W|S) + P(W|H) )

The probability of a word being spammy, S, given a word, W, is equal to the probability of the frequency of word occurance in all spam messages in the training data, P(W|S)[if there were 300 messages marked spam and 25 occurances of the word 'purchase', the probability would be 25/300=.083], divided by the sum of P(W|S) and the frequency of word occurances in all ham messages in the training data, P(W|H).
----
 Combining individual probabilities

p(S) = (p1 * p2 ... pn) / ( (p1 * p2 ... * pn) + ( (1 - p1) * (1 - p2) ... * (1 - pn) ) )

The formula for summing the probability of a message being labeled as spam, p(S), is equal to the product of each word's probability of being spammy, p1 through pn, divided by the sum of products p1 through pn and the products (1 - p1) through (1 - pn).

----
----
   Sqlite3 SCHEMA as created by NaiveBayesDB
     table: counters
       column : counter INTEGER (the total number of containers parsed)
           value0: <the value of the counter; defaults to 0>
           value1: <the value of the counter; defaults to 0>
           value2: <the value of the counter; defaults to 0>

       column : name TEXT (used to index each row's counter value)
    ie. "select counter from counters where name='positive_counter'"
    (subclass NaiveBayesDB to change these naming constants)
           value0: "global_counter"
           value1: "positive_counter"
           value2: "negative_counter"

       column : description TEXT
           value0: <description of this table as input by NaiveBayesDB; defaults to ''>
           value1: <description of this table as input by NaiveBayesDB; defaults to ''>
           value2: <description of this table as input by NaiveBayesDB; defaults to ''>

     table: negative_classification
       column : tokens TEXT
           example_value0: "Viagra"
           example_value1: "Ciali$"
       column : count INTEGER
           example_value0: 3
           example_value1: 2

     table: positive_classification
       column : tokens TEXT
       column : count INTEGER"
----
Python used to create Sqlite3 database:
cursor.execute("create table counters (counter INTEGER, name TEXT, description TEXT)")
cursor.execute("insert into counters VALUES (0, 'global_counter', ?)", (global_description,))
cursor.execute("insert into counters VALUES (0, 'positive_counter', ?)", (positive_description,))
cursor.execute("insert into counters VALUES (0, 'negative_counter', ?)", (negative_description,))
cursor.execute("create table negative_classification (token TEXT UNIQUE, count INTEGER)")
cursor.execute("create table positive_classification (token TEXT UNIQUE, count INTEGER)")
----
----

